{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V0jYMkRIsx5"
      },
      "source": [
        "# 16x Image Enlargement using a Super Resolution Generative Adversarial Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHBclPMusF-M",
        "outputId": "1d1ddeb2-89ea-40b5-8be5-f691e648f7c3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, LeakyReLU, Dropout, BatchNormalization, GlobalAveragePooling2D, UpSampling2D, Add, PReLU, Lambda\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "# If using colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ88QI_oUU_J",
        "outputId": "34979b31-0462-4f02-cd33-d05eed4bb8b3"
      },
      "outputs": [],
      "source": [
        "def residual_block(x, filters=64):\n",
        "    shortcut = x\n",
        "    x = Conv2D(filters, (3,3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = PReLU(shared_axes=[1,2])(x)\n",
        "    x = Conv2D(filters, (3,3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([shortcut, x])\n",
        "    return x\n",
        "\n",
        "def upsample_pixel_shuffle(x, filters=64, scale=2):\n",
        "    x = Conv2D(filters * (scale**2), (3,3), padding='same')(x)\n",
        "\n",
        "    x = tf.keras.layers.UpSampling2D(size=scale, interpolation='nearest')(x)\n",
        "    x = Conv2D(filters, (3,3), padding='same')(x)\n",
        "    x = PReLU(shared_axes=[1,2])(x)\n",
        "    return x\n",
        "\n",
        "def build_generator(input_shape=(96,96,3), num_res_blocks=8):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(64, (9,9), padding='same')(inputs)\n",
        "    x = PReLU(shared_axes=[1,2])(x)\n",
        "    skip_connection = x\n",
        "\n",
        "    for _ in range(num_res_blocks):\n",
        "        x = residual_block(x, 64)\n",
        "\n",
        "    x = Conv2D(64, (3,3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, skip_connection])\n",
        "\n",
        "    x = upsample_pixel_shuffle(x, 256, scale=2)  # 96x96 -> 192x192\n",
        "    x = upsample_pixel_shuffle(x, 256, scale=2)  # 192x192 -> 384x384\n",
        "\n",
        "    outputs = Conv2D(3, (9,9), padding='same', activation='tanh')(x)\n",
        "\n",
        "    return Model(inputs, outputs, name=\"Generator\")\n",
        "\n",
        "def build_discriminator(input_shape=(384, 384, 3)):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(64, (3,3), strides=1, padding='same')(inputs)\n",
        "    x = LeakyReLU(negative_slope=0.2)(x)\n",
        "\n",
        "    x = Conv2D(64, (1,1), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(negative_slope=0.2)(x)\n",
        "\n",
        "    x = Conv2D(128, (3,3), strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(negative_slope=0.2)(x)\n",
        "\n",
        "    x = Conv2D(128, (1,1), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(negative_slope=0.2)(x)\n",
        "\n",
        "    x = Conv2D(256, (3,3), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(negative_slope=0.2)(x)\n",
        "\n",
        "    x = Conv2D(512, (3,3), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(negative_slope=0.2)(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    return Model(inputs, outputs, name=\"Discriminator\")\n",
        "\n",
        "disc_model = build_discriminator()\n",
        "gen_model = build_generator()\n",
        "print(gen_model.count_params(), disc_model.count_params())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Nju8YhpDilx",
        "outputId": "c187f0d7-2253-4177-da84-24bd8567e9ca"
      },
      "outputs": [],
      "source": [
        "def build_vgg_feature_extractor(layer_name='block5_conv4'):\n",
        "    vgg = VGG19(weights='imagenet', include_top=False)\n",
        "    vgg.trainable = False\n",
        "    out = vgg.get_layer(layer_name).output\n",
        "    model = Model(vgg.input, out)\n",
        "    model.trainable = False\n",
        "    return model\n",
        "\n",
        "vgg_feat = build_vgg_feature_extractor('block5_conv4')\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "def content_loss_fn(hr, sr):\n",
        "    def preprocess_for_vgg(x):\n",
        "        x = (x + 1.0) * 127.5  # [-1,1] -> [0,255]\n",
        "        return tf.keras.applications.vgg19.preprocess_input(x)\n",
        "    hr_proc = preprocess_for_vgg(hr)\n",
        "    sr_proc = preprocess_for_vgg(sr)\n",
        "    hr_feat = vgg_feat(hr_proc)\n",
        "    sr_feat = vgg_feat(sr_proc)\n",
        "    return mse(hr_feat, sr_feat)\n",
        "\n",
        "print(\"D trainable weights:\", len(disc_model.trainable_weights))\n",
        "\n",
        "content_weight = 1e-1\n",
        "adv_weight = 5e-3\n",
        "n_critic = 2 # Discriminator updates per Generator update\n",
        "\n",
        "d_optimizer = tf.keras.optimizers.Adam(1e-5, 0.5)\n",
        "g_optimizer = tf.keras.optimizers.Adam(1e-4, 0.5)\n",
        "\n",
        "@tf.function(reduce_retracing=True)\n",
        "def train_step(lr_batch, hr_batch):\n",
        "    batch_size = tf.shape(lr_batch)[0]\n",
        "    real_labels = tf.ones((batch_size, 1)) * 0.9\n",
        "    fake_labels = tf.zeros((batch_size, 1)) + 0.1\n",
        "\n",
        "    for _ in range(n_critic):\n",
        "        with tf.GradientTape() as tape_d:\n",
        "            sr = gen_model(lr_batch, training=True)\n",
        "            real_out = disc_model(hr_batch, training=True)\n",
        "            fake_out = disc_model(sr, training=True)\n",
        "\n",
        "            # LSGAN discriminator loss: 0.5 * ( (D(x)-1)^2 + (D(G)-0)^2 )\n",
        "            d_loss_real = mse(real_labels, real_out)\n",
        "            d_loss_fake = mse(fake_labels, fake_out)\n",
        "            d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "\n",
        "        grads = tape_d.gradient(d_loss, disc_model.trainable_variables)\n",
        "        d_optimizer.apply_gradients(zip(grads, disc_model.trainable_variables))\n",
        "\n",
        "    with tf.GradientTape() as tape_g:\n",
        "        sr = gen_model(lr_batch, training=True)\n",
        "        fake_out_for_g = disc_model(sr, training=False)\n",
        "        adv_loss = mse(real_labels, fake_out_for_g)\n",
        "        cont_loss = content_loss_fn(hr_batch, sr)\n",
        "        g_loss = content_weight * cont_loss + adv_weight * adv_loss\n",
        "\n",
        "    grads = tape_g.gradient(g_loss, gen_model.trainable_variables)\n",
        "    g_optimizer.apply_gradients(zip(grads, gen_model.trainable_variables))\n",
        "\n",
        "    return d_loss, g_loss, cont_loss, adv_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S12FD3RG9Pkv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# checkpoint_dir = '/content/drive/MyDrive/models/checkpoints' If using colab\n",
        "checkpoint_dir = '/models/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    generator_optimizer=g_optimizer,\n",
        "    discriminator_optimizer=d_optimizer,\n",
        "    generator=gen_model,\n",
        "    discriminator=disc_model\n",
        ")\n",
        "\n",
        "checkpoint_manager = tf.train.CheckpointManager(\n",
        "    checkpoint,\n",
        "    directory=checkpoint_dir,\n",
        "    max_to_keep=5\n",
        ")\n",
        "\n",
        "def train_gan_full(lr_images, hr_images, epochs=1000, batch_size=2,\n",
        "                   save_interval=50, start_epoch=0):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((lr_images, hr_images)).shuffle(len(hr_images), reshuffle_each_iteration=True).take(100).batch(batch_size).prefetch(4)\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        d_losses = []\n",
        "        g_losses = []\n",
        "\n",
        "        for lr_batch, hr_batch in dataset:\n",
        "            d_loss, g_loss, cont_loss, adv_loss = train_step(lr_batch, hr_batch)\n",
        "            d_losses.append(d_loss.numpy())\n",
        "            g_losses.append(g_loss.numpy())\n",
        "        print(f\"Epoch {epoch+1}/{epochs} D_loss={np.mean(d_losses):.4f} G_loss={np.mean(g_losses):.4f} content={cont_loss.numpy():.4f} adv={adv_loss.numpy():.4f}\")\n",
        "        if (epoch + 1) % save_interval == 0:\n",
        "            save_path = checkpoint_manager.save()\n",
        "            print(f\"Checkpoint saved at epoch {epoch+1}: {save_path}\")\n",
        "            gen_model.save(f'{checkpoint_dir}/gen_epoch_{epoch+1}.keras')\n",
        "            disc_model.save(f'{checkpoint_dir}/disc_epoch_{epoch+1}.keras')\n",
        "\n",
        "    checkpoint_manager.save()\n",
        "    gen_model.save(f'{checkpoint_dir}/gen_final.keras')\n",
        "    disc_model.save(f'{checkpoint_dir}/disc_final.keras')\n",
        "    print(\"Training complete. Final model saved.\")\n",
        "\n",
        "def resume_training(lr_images, hr_images, epochs=2000, batch_size=2):\n",
        "    \"\"\"Load latest checkpoint and resume training\"\"\"\n",
        "\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "    if latest_checkpoint:\n",
        "        status = checkpoint.restore(latest_checkpoint)\n",
        "        print(f\"Restored from checkpoint: {latest_checkpoint}\")\n",
        "\n",
        "        try:\n",
        "            start_epoch = int(latest_checkpoint.split('-')[-1])\n",
        "            #print(f\"Resuming from epoch {start_epoch}\")\n",
        "        except:\n",
        "            start_epoch = 0\n",
        "            print(\"Could not determine epoch number, starting from 0\")\n",
        "    else:\n",
        "        print(\"No checkpoint found. Starting fresh training.\")\n",
        "        start_epoch = 0\n",
        "\n",
        "    train_gan_full(lr_images, hr_images, epochs, batch_size,\n",
        "                   save_interval=50, start_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4bKJI4SWGVv",
        "outputId": "42f84282-92a1-4411-fe45-7cf97349f365"
      },
      "outputs": [],
      "source": [
        "def _load_single_pair(args):\n",
        "    name, lr_dir, hr_dir, lr_size, hr_size = args\n",
        "    lr_path = os.path.join(lr_dir, f\"{name}.png\")\n",
        "    hr_path = os.path.join(hr_dir, f\"{name}.png\")\n",
        "    lr = Image.open(lr_path).convert(\"RGB\")\n",
        "    lr = lr.resize(lr_size, Image.LANCZOS)\n",
        "    lr_arr = np.array(lr, dtype=np.float32) / 127.5 - 1.0\n",
        "    hr = Image.open(hr_path).convert(\"RGB\")\n",
        "    hr = hr.resize(hr_size, Image.LANCZOS)\n",
        "    hr_arr = np.array(hr, dtype=np.float32) / 127.5 - 1.0\n",
        "\n",
        "    return lr_arr, hr_arr\n",
        "\n",
        "def load_image_pairs_fast(lr_dir, hr_dir, pair_filenames, lr_size=(96,96), hr_size=(384,384), save_dir=None, max_workers=8):\n",
        "    lr_images = []\n",
        "    hr_images = []\n",
        "\n",
        "    args_list = [(name, lr_dir, hr_dir, lr_size, hr_size) for name in pair_filenames]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = list(tqdm(executor.map(_load_single_pair, args_list), total=len(args_list), desc=\"Loading images\"))\n",
        "\n",
        "    for lr_arr, hr_arr in results:\n",
        "        lr_images.append(lr_arr)\n",
        "        hr_images.append(hr_arr)\n",
        "\n",
        "    lr_images = np.stack(lr_images)\n",
        "    hr_images = np.stack(hr_images)\n",
        "\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        lr_npz_path = os.path.join(save_dir, \"lr_images.npz\")\n",
        "        hr_npz_path = os.path.join(save_dir, \"hr_images.npz\")\n",
        "\n",
        "        np.savez_compressed(lr_npz_path, lr_images)\n",
        "        np.savez_compressed(hr_npz_path, hr_images)\n",
        "\n",
        "        print(f\"Saved LR to {lr_npz_path}\")\n",
        "        print(f\"Saved HR to {hr_npz_path}\")\n",
        "\n",
        "    return lr_images, hr_images\n",
        "\n",
        "lr_dir = \"processed/lr\"\n",
        "hr_dir = \"processed/hr\"\n",
        "save_dir = \"processed\"\n",
        "\n",
        "lr_files = [f for f in os.listdir(lr_dir) if f.endswith(\".png\")]\n",
        "pair_filenames = [os.path.splitext(f)[0] for f in lr_files]\n",
        "\n",
        "lr_images, hr_images = load_image_pairs_fast(lr_dir, hr_dir, pair_filenames, save_dir=save_dir)\n",
        "\n",
        "print(\"LR shape:\", lr_images.shape)\n",
        "print(\"HR shape:\", hr_images.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbs8nMiH4odf",
        "outputId": "9db4692a-9d7d-467e-953f-39a171685144"
      },
      "outputs": [],
      "source": [
        "def load_npz_images(lr_npz_path, hr_npz_path):\n",
        "    if not os.path.exists(lr_npz_path):\n",
        "        raise FileNotFoundError(f\"LR file not found: {lr_npz_path}\")\n",
        "    if not os.path.exists(hr_npz_path):\n",
        "        raise FileNotFoundError(f\"HR file not found: {hr_npz_path}\")\n",
        "\n",
        "    lr_data = np.load(lr_npz_path)\n",
        "    hr_data = np.load(hr_npz_path)\n",
        "\n",
        "    lr_images = lr_data['arr_0']\n",
        "    hr_images = hr_data['arr_0']\n",
        "\n",
        "    print(f\"Loaded LR images: {lr_images.shape}\")\n",
        "    print(f\"Loaded HR images: {hr_images.shape}\")\n",
        "\n",
        "    return lr_images, hr_images\n",
        "\n",
        "# Colab\n",
        "# lr_npz = \"/content/drive/MyDrive/processed/lr_images.npz\"\n",
        "# hr_npz = \"/content/drive/MyDrive/processed/hr_images.npz\"\n",
        "\n",
        "lr_npz = \"processed/lr_images.npz\"\n",
        "hr_npz = \"processed/hr_images.npz\"\n",
        "\n",
        "lr_images, hr_images = load_npz_images(lr_npz, hr_npz)\n",
        "\n",
        "print(\"Subset LR shape:\", lr_images.shape)\n",
        "print(\"Subset HR shape:\", hr_images.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L44nBGOanM1C",
        "outputId": "2cf2af5c-57c4-4aba-814e-4aecf306b474"
      },
      "outputs": [],
      "source": [
        "x = input(\"T for training, R for resume: \")\n",
        "if x.lower()==\"t\":\n",
        "    train_gan_full(lr_images, hr_images, epochs=100, batch_size=4, save_interval=50)\n",
        "if x.lower()==\"r\":\n",
        "    resume_training(lr_images, hr_images, epochs=100, batch_size=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxessSWK0q91"
      },
      "outputs": [],
      "source": [
        "# Colab\n",
        "# gen_model.save('/content/drive/MyDrive/models/srgan_generator.keras')\n",
        "# disc_model.save('/content/drive/MyDrive/models/srgan_discriminator.keras')\n",
        "gen_model.save('models/srgan_generator.keras')\n",
        "disc_model.save('models/srgan_discriminator.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONSYE8fA1KEs",
        "outputId": "0841a6db-cc57-4f8f-a5e5-fc75ac0bc2e9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "import time\n",
        "\n",
        "# gen = load_model('/content/drive/MyDrive/models/srgan_generator.keras', compile=False, safe_mode=False)\n",
        "gen = load_model('/models/srgan_generator.keras', compile=False, safe_mode=False)\n",
        "timestamp = int(time.time())\n",
        "def preprocess_img(path):\n",
        "    img = Image.open(path)\n",
        "    arr = (np.array(img, dtype=np.float32) / 127.5) - 1.0\n",
        "    return arr[np.newaxis, ...]\n",
        "test_file_name = '/content/1750465599751830_p000'\n",
        "lr_input = preprocess_img(f'{test_file_name}.png')\n",
        "sr_output = gen.predict(lr_input)\n",
        "\n",
        "sr_img = ((sr_output[0] + 1.0) * 127.5).clip(0,255).astype(np.uint8)\n",
        "Image.fromarray(sr_img).save(f'/{test_file_name}super_resolved{timestamp}.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7CFOzHiZwlqC",
        "outputId": "04697f07-0bd1-40df-cba6-70a52272f70a"
      },
      "outputs": [],
      "source": [
        "timestamp = int(time.time())\n",
        "# gen = load_model('/content/drive/MyDrive/models/srgan_generator.keras', compile=False, safe_mode=False)\n",
        "gen = load_model('/models/srgan_generator.keras', compile=False, safe_mode=False)\n",
        "def comprehensive_evaluation(lr_test, hr_test, gen_model):\n",
        "    sr_test = gen_model.predict(lr_test, verbose=0)\n",
        "\n",
        "    hr_01 = tf.cast((hr_test + 1.0) / 2.0, tf.float32)\n",
        "    sr_01 = tf.cast((sr_test + 1.0) / 2.0, tf.float32)\n",
        "    lr_01 = tf.cast((lr_test + 1.0) / 2.0, tf.float32)\n",
        "\n",
        "    psnr_vals = tf.image.psnr(sr_01, hr_01, max_val=1.0).numpy()\n",
        "    ssim_vals = tf.image.ssim(sr_01, hr_01, max_val=1.0).numpy()\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"SRGAN Evaluation Results\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Mean PSNR: {np.mean(psnr_vals):.2f} dB\")\n",
        "    print(f\"Mean SSIM: {np.mean(ssim_vals):.4f}\")\n",
        "    print(f\"Median PSNR: {np.median(psnr_vals):.2f} dB\")\n",
        "    print(f\"Median SSIM: {np.median(ssim_vals):.4f}\")\n",
        "    print(f\"Best PSNR: {np.max(psnr_vals):.2f} dB\")\n",
        "    print(f\"Worst PSNR: {np.min(psnr_vals):.2f} dB\")\n",
        "    print(\"=\" * 50)\n",
        "    n_images = len(lr_test)\n",
        "    n_cols = 5\n",
        "    n_rows = (n_images + n_cols - 1) // n_cols\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "    for i in range(n_images):\n",
        "        lr_upscaled = tf.image.resize(\n",
        "            lr_01[i:i+1],\n",
        "            [384, 384],\n",
        "            method='nearest'\n",
        "        )[0].numpy()\n",
        "        comparison = np.hstack([\n",
        "            lr_upscaled, # Low res\n",
        "            sr_01[i].numpy(), # Super resolved\n",
        "            hr_01[i].numpy() # Ground truth image\n",
        "        ])\n",
        "        axes[i].imshow(comparison)\n",
        "        axes[i].set_title(\n",
        "            f'Image {i+1}\\nPSNR: {psnr_vals[i]:.2f} dB | SSIM: {ssim_vals[i]:.3f}',\n",
        "            fontsize=10\n",
        "        )\n",
        "        axes[i].axis('off')\n",
        "    for i in range(n_images, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'/content/drive/MyDrive/colab_data/all_test_results{timestamp}.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return psnr_vals, ssim_vals\n",
        "# The ones which I've checked often for past versions are 400:420\n",
        "psnr_scores, ssim_scores = comprehensive_evaluation(lr_images[400:420], hr_images[400:420], gen_model=gen)\n",
        "\n",
        "print(\"\\nPSNR scores:\", psnr_scores)\n",
        "print(\"SSIM scores:\", ssim_scores)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
